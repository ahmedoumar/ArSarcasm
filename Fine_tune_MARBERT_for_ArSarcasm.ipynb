{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "Copy of Fine-tune MARBERT from checkpoint.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahmedoumar/ArSarcasm/blob/master/Fine_tune_MARBERT_for_ArSarcasm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUiwkRv-XAkW"
      },
      "source": [
        "# Download MARBERT checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssGpu-zumweV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e79b31ea-9e8d-426b-b1e6-6c2012fab054"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8wBRd5zXAkX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ba8c17c-d76f-4a2e-a578-9673e9412e41"
      },
      "source": [
        "!wget https://huggingface.co/UBC-NLP/MARBERT/resolve/main/MARBERT_pytorch_verison.tar.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-13 10:49:27--  https://huggingface.co/UBC-NLP/MARBERT/resolve/main/MARBERT_pytorch_verison.tar.gz\n",
            "Resolving huggingface.co (huggingface.co)... 34.201.172.85\n",
            "Connecting to huggingface.co (huggingface.co)|34.201.172.85|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/UBC-NLP/MARBERT/85bfec76f38cba4bc2e6cd02a959016de37ba93de4c850a7d175811dce4e8adc [following]\n",
            "--2021-04-13 10:49:27--  https://cdn-lfs.huggingface.co/UBC-NLP/MARBERT/85bfec76f38cba4bc2e6cd02a959016de37ba93de4c850a7d175811dce4e8adc\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 52.84.162.46, 52.84.162.9, 52.84.162.75, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|52.84.162.46|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 607066087 (579M) [application/x-gzip]\n",
            "Saving to: ‘MARBERT_pytorch_verison.tar.gz’\n",
            "\n",
            "MARBERT_pytorch_ver 100%[===================>] 578.94M   218MB/s    in 2.7s    \n",
            "\n",
            "2021-04-13 10:49:30 (218 MB/s) - ‘MARBERT_pytorch_verison.tar.gz’ saved [607066087/607066087]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FYvgJevXAkY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c3a1b9e-8e9e-421b-953d-6d8c26ce8385"
      },
      "source": [
        "!tar -xvf MARBERT_pytorch_verison.tar.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MARBERT_pytorch_verison/\n",
            "MARBERT_pytorch_verison/pytorch_model.bin\n",
            "MARBERT_pytorch_verison/config.json\n",
            "MARBERT_pytorch_verison/vocab.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUeiBs3tXAkZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7385f395-60f3-4f10-d9af-f8970cea28cd"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/UBC-NLP/marbert/main/examples/UBC_AJGT_final_shuffled_train.tsv\n",
        "!wget https://raw.githubusercontent.com/UBC-NLP/marbert/main/examples/UBC_AJGT_final_shuffled_test.tsv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-13 10:49:39--  https://raw.githubusercontent.com/UBC-NLP/marbert/main/examples/UBC_AJGT_final_shuffled_train.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 149495 (146K) [text/plain]\n",
            "Saving to: ‘UBC_AJGT_final_shuffled_train.tsv’\n",
            "\n",
            "UBC_AJGT_final_shuf 100%[===================>] 145.99K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2021-04-13 10:49:40 (9.32 MB/s) - ‘UBC_AJGT_final_shuffled_train.tsv’ saved [149495/149495]\n",
            "\n",
            "--2021-04-13 10:49:40--  https://raw.githubusercontent.com/UBC-NLP/marbert/main/examples/UBC_AJGT_final_shuffled_test.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 36901 (36K) [text/plain]\n",
            "Saving to: ‘UBC_AJGT_final_shuffled_test.tsv’\n",
            "\n",
            "UBC_AJGT_final_shuf 100%[===================>]  36.04K  --.-KB/s    in 0.003s  \n",
            "\n",
            "2021-04-13 10:49:40 (13.9 MB/s) - ‘UBC_AJGT_final_shuffled_test.tsv’ saved [36901/36901]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXM2ZsS1XAka"
      },
      "source": [
        "!mkdir -p AJGT\n",
        "!mv UBC_AJGT_final_shuffled_train.tsv ./AJGT/UBC_AJGT_final_shuffled_train.tsv\n",
        "!mv UBC_AJGT_final_shuffled_test.tsv ./AJGT/UBC_AJGT_final_shuffled_test.tsv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2pkLcyfXAka",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab610298-296e-4970-fe55-d9f877b48f84"
      },
      "source": [
        "!pip install GPUtil pytorch_pretrained_bert transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting GPUtil\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n",
            "Collecting pytorch_pretrained_bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 6.5MB/s \n",
            "\u001b[?25hCollecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/81/91/61d69d58a1af1bd81d9ca9d62c90a6de3ab80d77f27c5df65d9a2c1f5626/transformers-4.5.0-py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2MB 10.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.8.1+cu101)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.19.5)\n",
            "Collecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/60/78919d8b178668aac44b5d5f4fbe660880179ada1e9000cf3ee3bfcb6421/boto3-1.17.50.tar.gz (99kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 11.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.1)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 34.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/cd/342e584ee544d044fb573ae697404ce22ede086c9e87ce5960772084cad0/sacremoses-0.0.44.tar.gz (862kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 43.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.7.4.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Collecting botocore<1.21.0,>=1.20.50\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/ae/e7e003597f954283f90f21891bda64bab0fc1738951aeb09a7c798ef0a60/botocore-1.20.50-py2.py3-none-any.whl (7.4MB)\n",
            "\u001b[K     |████████████████████████████████| 7.4MB 53.7MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Collecting s3transfer<0.4.0,>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/14/0b4be62b65c52d6d1c442f24e02d2a9889a73d3c352002e14c70f84a679f/s3transfer-0.3.6-py2.py3-none-any.whl (73kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 12.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.50->boto3->pytorch_pretrained_bert) (2.8.1)\n",
            "Building wheels for collected packages: GPUtil, boto3, sacremoses\n",
            "  Building wheel for GPUtil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for GPUtil: filename=GPUtil-1.4.0-cp37-none-any.whl size=7411 sha256=202d6c7372f8a322cedd74b1ba321adf0f0ac38f36747b9b7cb6505dc4c2584b\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n",
            "  Building wheel for boto3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for boto3: filename=boto3-1.17.50-py2.py3-none-any.whl size=128779 sha256=0d9fb3d5e7159fea611c3df15b259763b30b03dbe816722beea07cd3d167eb50\n",
            "  Stored in directory: /root/.cache/pip/wheels/28/e5/43/ef6fc36c3008477a35f9324c0e490c7aa20f7b51993a388267\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.44-cp37-none-any.whl size=886084 sha256=da773e59a2645456932bba9cf036c7fe65cd2b23c1b50cd25fafa304ef5e9b2c\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/fb/c0/13ab4d63d537658f448366744654323077c4d90069b6512f3c\n",
            "Successfully built GPUtil boto3 sacremoses\n",
            "\u001b[31mERROR: botocore 1.20.50 has requirement urllib3<1.27,>=1.25.4, but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: GPUtil, jmespath, botocore, s3transfer, boto3, pytorch-pretrained-bert, tokenizers, sacremoses, transformers\n",
            "Successfully installed GPUtil-1.4.0 boto3-1.17.50 botocore-1.20.50 jmespath-0.10.0 pytorch-pretrained-bert-0.6.2 s3transfer-0.3.6 sacremoses-0.0.44 tokenizers-0.10.2 transformers-4.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etLEynLKmdgA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f88aaa3e-dc05-435d-edde-3d5194943a15"
      },
      "source": [
        "!git clone https://github.com/iabufarha/ArSarcasm.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'ArSarcasm'...\n",
            "remote: Enumerating objects: 85, done.\u001b[K\n",
            "remote: Counting objects: 100% (85/85), done.\u001b[K\n",
            "remote: Compressing objects: 100% (81/81), done.\u001b[K\n",
            "remote: Total 85 (delta 37), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (85/85), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xBm7opGXAkb"
      },
      "source": [
        "# Fine-tuning code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAtnJG1VnlNb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf069f62-8c9e-4425-bccc-21b4820f4595"
      },
      "source": [
        "!pip3 install sentencepiece"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 9.1MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.95\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9oOXwzeXXAkb"
      },
      "source": [
        "# (1)load libraries \n",
        "import json, sys, regex\n",
        "import torch\n",
        "import GPUtil\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertConfig, BertAdam, BertForSequenceClassification\n",
        "from tqdm import tqdm, trange\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, classification_report, confusion_matrix\n",
        "##----------------------------------------------------\n",
        "from transformers import *\n",
        "from transformers import XLMRobertaConfig\n",
        "from transformers import XLMRobertaModel\n",
        "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
        "from transformers import XLMRobertaForSequenceClassification, XLMRobertaTokenizer, XLMRobertaModel\n",
        "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from transformers import AutoTokenizer, AutoModel\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQWKMrXPXAkc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c15bee8-8081-43b6-eb82-82be5a6fc04c"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print (\"your device \", device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "your device  cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKujHwr5XAkd"
      },
      "source": [
        "\n",
        "def create_label2ind_file(file, label_col):\n",
        "\tlabels_json={}\n",
        "\t#load train_dev_test file\n",
        "\tdf = pd.read_csv(file, sep=\",\")\n",
        "\tdf.head(5)\n",
        "\t#get labels and sort it A-Z\n",
        "\tlabels = df[label_col].unique()\n",
        "\tlabels.sort()\n",
        "\t#convert labels to indexes\n",
        "\tfor idx in range(0, len(labels)):\n",
        "\t\tlabels_json[labels[idx]]=idx\n",
        "\t#save labels with indexes to file\n",
        "\twith open(label2idx_file, 'w') as json_file:\n",
        "\t\tjson.dump(labels_json, json_file)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNZxr8m6XAkd"
      },
      "source": [
        "\n",
        "def data_prepare_BERT(file_path, lab2ind, tokenizer, content_col, label_col, MAX_LEN):\n",
        "\t# Use pandas to load dataset\n",
        "\tdf = pd.read_csv(file_path, delimiter='\\t', header=0); df[['sarcasm', 'tweet']] = df['sarcasm,tweet'].str.split(',', 1, expand=True)\n",
        "\tdf = df[df[content_col].notnull()]\n",
        "\tdf = df[df[label_col].notnull()]\n",
        "\tprint(\"Data size \", df.shape)\n",
        "\t# Create sentence and label lists\n",
        "\tsentences = df[content_col].values\n",
        "\tsentences = [\"[CLS] \" + sentence+ \" [SEP]\" for sentence in sentences]\n",
        "\tprint (\"The first sentence:\")\n",
        "\tprint (sentences[0])\n",
        "\t# Create sentence and label lists\n",
        "\tlabels = df[label_col].values\n",
        "\t#print (labels)\n",
        "\tlabels = [lab2ind[i] for i in labels]\n",
        "\t# Import the BERT tokenizer, used to convert our text into tokens that correspond to BERT's vocabulary.\n",
        "\ttokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "\tprint (\"Tokenize the first sentence:\")\n",
        "\tprint (tokenized_texts[0])\n",
        "\t#print(\"Label is \", labels[0])\n",
        "\t# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
        "\tinput_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "\tprint (\"Index numbers of the first sentence:\")\n",
        "\tprint (input_ids[0])\n",
        "\t# Pad our input seqeunce to the fixed length (i.e., max_len) with index of [PAD] token\n",
        "\t# ~ input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\tpad_ind = tokenizer.convert_tokens_to_ids(['[PAD]'])[0]\n",
        "\tinput_ids = pad_sequences(input_ids, maxlen=MAX_LEN+2, dtype=\"long\", truncating=\"post\", padding=\"post\", value=pad_ind)\n",
        "\tprint (\"Index numbers of the first sentence after padding:\\n\",input_ids[0])\n",
        "\t# Create attention masks\n",
        "\tattention_masks = []\n",
        "\t# Create a mask of 1s for each token followed by 0s for padding\n",
        "\tfor seq in input_ids:\n",
        "\t\tseq_mask = [float(i > 0) for i in seq]\n",
        "\t\tattention_masks.append(seq_mask)\n",
        "\t# Convert all of our data into torch tensors, the required datatype for our model\n",
        "\tinputs = torch.tensor(input_ids)\n",
        "\tlabels = torch.tensor(labels)\n",
        "\tmasks = torch.tensor(attention_masks)\n",
        "\treturn inputs, labels, masks\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdPCPv8VXAke"
      },
      "source": [
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "# def flat_accuracy(preds, labels):\n",
        "#\t  pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "#\t  labels_flat = labels.flatten()\n",
        "#\t  return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "def flat_pred(preds, labels):\n",
        "\tpred_flat = np.argmax(preds, axis=1).flatten()\n",
        "\tlabels_flat = labels.flatten()\n",
        "\treturn pred_flat.tolist(), labels_flat.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vyvc8JoXAke"
      },
      "source": [
        "\n",
        "def train(model, iterator, optimizer, scheduler, criterion):\n",
        "\t\n",
        "\tmodel.train()\n",
        "\tepoch_loss = 0\n",
        "\tfor i, batch in enumerate(iterator):\n",
        "\t\t# Add batch to GPU\n",
        "\t\tbatch = tuple(t.to(device) for t in batch)\n",
        "\t\t# Unpack the inputs from our dataloader\n",
        "\t\tinput_ids, input_mask, labels = batch\n",
        "\t\toutputs = model(input_ids, input_mask, labels=labels)\n",
        "\t\tloss, logits = outputs[:2]\n",
        "\t\t# delete used variables to free GPU memory\n",
        "\t\tdel batch, input_ids, input_mask, labels\n",
        "\t\toptimizer.zero_grad()\n",
        "\t\tif torch.cuda.device_count() == 1:\n",
        "\t\t\tloss.backward()\n",
        "\t\t\tepoch_loss += loss.cpu().item()\n",
        "\t\telse:\n",
        "\t\t\tloss.sum().backward()\n",
        "\t\t\tepoch_loss += loss.sum().cpu().item()\n",
        "\t\toptimizer.step()\n",
        "\t\ttorch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)  # Gradient clipping is not in AdamW anymore\n",
        "\t\t# optimizer.step()\n",
        "\t\tscheduler.step()\n",
        "\t# free GPU memory\n",
        "\tif device == 'cuda':\n",
        "\t\ttorch.cuda.empty_cache()\n",
        "\treturn epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hjwa6a3bXAkf"
      },
      "source": [
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "\tmodel.eval()\n",
        "\tepoch_loss = 0\n",
        "\tall_pred=[]\n",
        "\tall_label = []\n",
        "\twith torch.no_grad():\n",
        "\t\tfor i, batch in enumerate(iterator):\n",
        "\t\t\t# Add batch to GPU\n",
        "\t\t\tbatch = tuple(t.to(device) for t in batch)\n",
        "\t\t\t# Unpack the inputs from our dataloader\n",
        "\t\t\tinput_ids, input_mask, labels = batch\n",
        "\t\t\toutputs = model(input_ids, input_mask, labels=labels)\n",
        "\t\t\tloss, logits = outputs[:2]\n",
        "\t\t\t# delete used variables to free GPU memory\n",
        "\t\t\tdel batch, input_ids, input_mask\n",
        "\t\t\tif torch.cuda.device_count() == 1:\n",
        "\t\t\t\tepoch_loss += loss.cpu().item()\n",
        "\t\t\telse:\n",
        "\t\t\t\tepoch_loss += loss.sum().cpu().item()\n",
        "\t\t\t# identify the predicted class for each example in the batch\n",
        "\t\t\tprobabilities, predicted = torch.max(logits.cpu().data, 1)\n",
        "\t\t\t# put all the true labels and predictions to two lists\n",
        "\t\t\tall_pred.extend(predicted)\n",
        "\t\t\tall_label.extend(labels.cpu())\n",
        "\taccuracy = accuracy_score(all_label, all_pred)\n",
        "\tf1score = f1_score(all_label, all_pred, average='macro') \n",
        "\trecall = recall_score(all_label, all_pred, average='macro')\n",
        "\tprecision = precision_score(all_label, all_pred, average='macro')\n",
        "\treport = classification_report(all_label, all_pred)\n",
        "\treturn (epoch_loss / len(iterator)), accuracy, f1score, recall, precision\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nWmg6P-XAkf"
      },
      "source": [
        "\n",
        "def fine_tuning(config):\n",
        "\t#---------------------------------------\n",
        "\tprint (\"[INFO] step (1) load train_test config file\")\n",
        "\t# config_file = open(config_file, 'r', encoding=\"utf8\")\n",
        "\t# config = json.load(config_file)\n",
        "\ttask_name = config[\"task_name\"]\n",
        "\tcontent_col = config[\"content_col\"]\n",
        "\tlabel_col = config[\"label_col\"]\n",
        "\ttrain_file = config[\"data_dir\"]+config[\"train_file\"]\n",
        "\tdev_file = config[\"data_dir\"]+config[\"dev_file\"]\n",
        "\tsortby = config[\"sortby\"]\n",
        "\tmax_seq_length= int(config[\"max_seq_length\"])\n",
        "\tbatch_size = int(config[\"batch_size\"])\n",
        "\tlr_var = float(config[\"lr\"])\n",
        "\tmodel_path = config['pretrained_model_path']\n",
        "\tnum_epochs = config['epochs'] # Number of training epochs (authors recommend between 2 and 4)\n",
        "\tglobal label2idx_file\n",
        "\tlabel2idx_file = config[\"data_dir\"]+config[\"task_name\"]+\"_labels-dict.json\"\n",
        "\t#-------------------------------------------------------\n",
        "\tprint (\"[INFO] step (2) convert labels2index\")\n",
        "\tcreate_label2ind_file(train_file, label_col)\n",
        "\tprint (label2idx_file)\n",
        "\t#---------------------------------------------------------\n",
        "\tprint (\"[INFO] step (3) check checkpoit directory and report file\")\n",
        "\tckpt_dir = config[\"data_dir\"]+task_name+\"_bert_ckpt/\"\n",
        "\treport = ckpt_dir+task_name+\"_report.tsv\"\n",
        "\tsorted_report = ckpt_dir+task_name+\"_report_sorted.tsv\"\n",
        "\tif not os.path.exists(ckpt_dir):\n",
        "\t\tos.mkdir(ckpt_dir)\n",
        "\t#-------------------------------------------------------\n",
        "\tprint (\"[INFO] step (4) load label to number dictionary\")\n",
        "\tlab2ind = json.load(open(label2idx_file))\n",
        "\tprint (\"[INFO] train_file\", train_file)\n",
        "\tprint (\"[INFO] dev_file\", dev_file)\n",
        "\tprint (\"[INFO] num_epochs\", num_epochs)\n",
        "\tprint (\"[INFO] model_path\", model_path)\n",
        "\tprint (\"max_seq_length\", max_seq_length, \"batch_size\", batch_size)\n",
        "\t#-------------------------------------------------------\n",
        "\tprint (\"[INFO] step (5) Use defined funtion to extract tokanize data\")\n",
        "\t# tokenizer from pre-trained BERT model\n",
        "\tprint (\"loading BERT setting\")\n",
        "\ttokenizer = BertTokenizer.from_pretrained(model_path)\n",
        "\ttrain_inputs, train_labels, train_masks = data_prepare_BERT(train_file, lab2ind, tokenizer,content_col, label_col, max_seq_length)\n",
        "\tvalidation_inputs, validation_labels, validation_masks = data_prepare_BERT(dev_file, lab2ind, tokenizer, content_col, label_col,max_seq_length)\n",
        "\t# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top.\n",
        "\tmodel = BertForSequenceClassification.from_pretrained(model_path, num_labels=len(lab2ind))\n",
        "\t#--------------------------------------\n",
        "\tprint (\"[INFO] step (6) Create an iterator of data with torch DataLoader.\")\n",
        "#\t\t  This helps save on memory during training because, unlike a for loop,\\\n",
        "#\t\t  with an iterator the entire dataset does not need to be loaded into memory\")\n",
        "\ttrain_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "\ttrain_dataloader = DataLoader(train_data, batch_size=batch_size)\n",
        "\t#---------------------------\n",
        "\tvalidation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "\tvalidation_dataloader = DataLoader(validation_data, batch_size=batch_size)\n",
        "\t#------------------------------------------\n",
        "\tprint (\"[INFO] step (7) run with parallel GPUs\")\n",
        "\tif torch.cuda.is_available():\n",
        "\t\tif torch.cuda.device_count() == 1:\n",
        "\t\t\tprint(\"Run\", \"with one GPU\")\n",
        "\t\t\tmodel = model.to(device)\n",
        "\t\telse:\n",
        "\t\t\tn_gpu = torch.cuda.device_count()\n",
        "\t\t\tprint(\"Run\", \"with\", n_gpu, \"GPUs with max 4 GPUs\")\n",
        "\t\t\tdevice_ids = GPUtil.getAvailable(limit = 4)\n",
        "\t\t\ttorch.backends.cudnn.benchmark = True\n",
        "\t\t\tmodel = model.to(device)\n",
        "\t\t\tmodel = nn.DataParallel(model, device_ids=device_ids)\n",
        "\telse:\n",
        "\t\tprint(\"Run\", \"with CPU\")\n",
        "\t\tmodel = model\n",
        "\t#---------------------------------------------------\n",
        "\tprint (\"[INFO] step (8) set Parameters, schedules, and loss function\")\n",
        "\tglobal max_grad_norm\n",
        "\tmax_grad_norm = 1.0\n",
        "\twarmup_proportion = 0.1\n",
        "\tnum_training_steps\t= len(train_dataloader) * num_epochs\n",
        "\tnum_warmup_steps = num_training_steps * warmup_proportion\n",
        "\t### In Transformers, optimizer and schedules are instantiated like this:\n",
        "\t# Note: AdamW is a class from the huggingface library\n",
        "\t# the 'W' stands for 'Weight Decay\"\n",
        "\toptimizer = AdamW(model.parameters(), lr=lr_var, correct_bias=False)\n",
        "\t# schedules\n",
        "\tscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)  # PyTorch scheduler\n",
        "\t# We use nn.CrossEntropyLoss() as our loss function. \n",
        "\tcriterion = nn.CrossEntropyLoss()\n",
        "\t#---------------------------------------------------\n",
        "\tprint (\"[INFO] step (9) start fine_tuning\")\n",
        "\tfor epoch in trange(num_epochs, desc=\"Epoch\"):\n",
        "        \n",
        "\t\ttrain_loss = train(model, train_dataloader, optimizer, scheduler, criterion)\t  \n",
        "\t\tprint(\"hello again\"); val_loss, val_acc, val_f1, val_recall, val_precision = evaluate(model, validation_dataloader, criterion)\n",
        "# \t\tprint (train_loss, val_acc)\n",
        "\t\t# Create checkpoint at end of each epoch\n",
        "\t\tif not os.path.exists(ckpt_dir + 'model_' + str(int(epoch + 1)) + '/'): os.mkdir(ckpt_dir + 'model_' + str(int(epoch + 1)) + '/')\n",
        "\t\tmodel.save_pretrained(ckpt_dir+ 'model_' + str(int(epoch + 1)) + '/')\n",
        "\t\tepoch_eval_results = {\"epoch_num\":int(epoch + 1),\"train_loss\":train_loss,\n",
        "\t\t\t\t\t  \"val_acc\":val_acc, \"val_recall\":val_recall, \"val_precision\":val_precision, \"val_f1\":val_f1,\"lr\":lr_var }\n",
        "\t\twith open(report,\"a\") as fOut:\n",
        "\t\t\tfOut.write(json.dumps(epoch_eval_results)+\"\\n\")\n",
        "\t\t\tfOut.flush()\n",
        "\t\t#------------------------------------\n",
        "\t\treport_df = pd.read_json(report, orient='records', lines=True)\n",
        "\t\treport_df.sort_values(by=[sortby],ascending=False, inplace=True)\n",
        "\t\treport_df.to_csv(sorted_report,sep=\"\\t\",index=False)\n",
        "\treturn report_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDnDyOFHiZvk"
      },
      "source": [
        "# Run fine-tuning for 5 epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEKPodVHpR6u"
      },
      "source": [
        "train = pd.read_csv('ArSarcasm/dataset/ArSarcasm_train.csv', index_col=False)\n",
        "test = pd.read_csv('ArSarcasm/dataset/ArSarcasm_test.csv', index_col=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x07IDfRmpsy9"
      },
      "source": [
        "train['sarcasm'].replace({True:\"Positive\", False:\"Negative\"}, inplace=True)\n",
        "test['sarcasm'].replace({True:\"Positive\", False:\"Negative\"}, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bsqf3Lel10gc"
      },
      "source": [
        "del train['dialect']\n",
        "del train['sentiment']\n",
        "del train['original_sentiment']\n",
        "del train['source']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fB66aoDk12zK"
      },
      "source": [
        "del test['dialect']\n",
        "del test['sentiment']\n",
        "del test['original_sentiment']\n",
        "del test['source']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMyHn_VSps8q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "1888494a-cff3-4f45-e534-79f8ef73a783"
      },
      "source": [
        "test.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sarcasm</th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Positive</td>\n",
              "      <td>\"@AbuEmad74241481 @Cesars2014 هههههه حزب الله ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Negative</td>\n",
              "      <td>\"RT @JannetForster: البنات اللي م صامو بقولكم ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Positive</td>\n",
              "      <td>اشارة رابعة اشبه بنار تحرق الانقلابيين</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Negative</td>\n",
              "      <td>\"@EGYPTAIR ماهي مميزات درجه بزنس علماً اني في ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Negative</td>\n",
              "      <td>ما لا تراه على التلفاز منافسة شديدة بين المرشح...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    sarcasm                                              tweet\n",
              "0  Positive  \"@AbuEmad74241481 @Cesars2014 هههههه حزب الله ...\n",
              "1  Negative  \"RT @JannetForster: البنات اللي م صامو بقولكم ...\n",
              "2  Positive             اشارة رابعة اشبه بنار تحرق الانقلابيين\n",
              "3  Negative  \"@EGYPTAIR ماهي مميزات درجه بزنس علماً اني في ...\n",
              "4  Negative  ما لا تراه على التلفاز منافسة شديدة بين المرشح..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pC8w_6SBpsnW"
      },
      "source": [
        "train.to_csv(r'train.csv', index=False)\n",
        "test.to_csv(r'test.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqr4SkOKq5Hq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "ea9cfeeb-9e71-4201-c5a3-cb4f8b06ca90"
      },
      "source": [
        "test.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sarcasm</th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Positive</td>\n",
              "      <td>\"@AbuEmad74241481 @Cesars2014 هههههه حزب الله ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Negative</td>\n",
              "      <td>\"RT @JannetForster: البنات اللي م صامو بقولكم ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Positive</td>\n",
              "      <td>اشارة رابعة اشبه بنار تحرق الانقلابيين</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Negative</td>\n",
              "      <td>\"@EGYPTAIR ماهي مميزات درجه بزنس علماً اني في ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Negative</td>\n",
              "      <td>ما لا تراه على التلفاز منافسة شديدة بين المرشح...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    sarcasm                                              tweet\n",
              "0  Positive  \"@AbuEmad74241481 @Cesars2014 هههههه حزب الله ...\n",
              "1  Negative  \"RT @JannetForster: البنات اللي م صامو بقولكم ...\n",
              "2  Positive             اشارة رابعة اشبه بنار تحرق الانقلابيين\n",
              "3  Negative  \"@EGYPTAIR ماهي مميزات درجه بزنس علماً اني في ...\n",
              "4  Negative  ما لا تراه على التلفاز منافسة شديدة بين المرشح..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ol6LaSAAXAki"
      },
      "source": [
        "\n",
        "config={\"task_name\": \"AJGT_MARBERT\", #output directory name\n",
        "             \"data_dir\": \"./\", #data directory\n",
        "             \"train_file\": \"train.csv\", #train file path\n",
        "             \"dev_file\": \"test.csv\", #dev file path or test file path\n",
        "             \"pretrained_model_path\": 'MARBERT_pytorch_verison', #MARBERT checkpoint path\n",
        "             \"epochs\": 10, #number of epochs\n",
        "             \"content_col\": \"tweet\", #text column\n",
        "             \"label_col\": \"sarcasm\", #label column\n",
        "             \"lr\": 2e-04, #learning rate\n",
        "              \"max_seq_length\": 128, #max sequance length\n",
        "              \"batch_size\": 16, #batch shize\n",
        "              \"sortby\":\"val_acc\"} #sort results based on val_acc or val_f1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jq9kOFoaXAkj"
      },
      "source": [
        "report_df = fine_tuning(config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5V9ZOdVNXAkk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "327893f8-531d-4d96-e628-e9141efdfb44"
      },
      "source": [
        "#This report is for the next hyperparameters:\n",
        "\"\"\"\n",
        "epochs = 5\n",
        "lr = 2e-06\n",
        "max_seq_length = 128\n",
        "batch_size = 16\n",
        "\"\"\"\n",
        "report_df.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>epoch_num</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>val_acc</th>\n",
              "      <th>val_recall</th>\n",
              "      <th>val_precision</th>\n",
              "      <th>val_f1</th>\n",
              "      <th>lr</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0.355991</td>\n",
              "      <td>0.872038</td>\n",
              "      <td>0.691481</td>\n",
              "      <td>0.783843</td>\n",
              "      <td>0.722881</td>\n",
              "      <td>0.000002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>0.152038</td>\n",
              "      <td>0.871090</td>\n",
              "      <td>0.766704</td>\n",
              "      <td>0.764246</td>\n",
              "      <td>0.765464</td>\n",
              "      <td>0.000002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>0.135789</td>\n",
              "      <td>0.871090</td>\n",
              "      <td>0.762040</td>\n",
              "      <td>0.764514</td>\n",
              "      <td>0.763266</td>\n",
              "      <td>0.000002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>0.185312</td>\n",
              "      <td>0.870142</td>\n",
              "      <td>0.775465</td>\n",
              "      <td>0.762221</td>\n",
              "      <td>0.768531</td>\n",
              "      <td>0.000002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>0.237678</td>\n",
              "      <td>0.867773</td>\n",
              "      <td>0.762389</td>\n",
              "      <td>0.758195</td>\n",
              "      <td>0.760260</td>\n",
              "      <td>0.000002</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   epoch_num  train_loss   val_acc  ...  val_precision    val_f1        lr\n",
              "0          1    0.355991  0.872038  ...       0.783843  0.722881  0.000002\n",
              "3          4    0.152038  0.871090  ...       0.764246  0.765464  0.000002\n",
              "4          5    0.135789  0.871090  ...       0.764514  0.763266  0.000002\n",
              "2          3    0.185312  0.870142  ...       0.762221  0.768531  0.000002\n",
              "1          2    0.237678  0.867773  ...       0.758195  0.760260  0.000002\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoRMJ72mhK9W"
      },
      "source": [
        "report_df2 = fine_tuning(config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "sKx9c_oPHkjF",
        "outputId": "dec96450-0055-4f06-d64c-c326505f94a9"
      },
      "source": [
        "#This report is for the next hyperparameters:\n",
        "\"\"\"\n",
        "epochs = 5\n",
        "lr = 2e-05\n",
        "max_seq_length = 128\n",
        "batch_size = 16\n",
        "\"\"\"\n",
        "report_df2.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>epoch_num</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>val_acc</th>\n",
              "      <th>val_recall</th>\n",
              "      <th>val_precision</th>\n",
              "      <th>val_f1</th>\n",
              "      <th>lr</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0.355991</td>\n",
              "      <td>0.872038</td>\n",
              "      <td>0.691481</td>\n",
              "      <td>0.783843</td>\n",
              "      <td>0.722881</td>\n",
              "      <td>0.000002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>0.152038</td>\n",
              "      <td>0.871090</td>\n",
              "      <td>0.766704</td>\n",
              "      <td>0.764246</td>\n",
              "      <td>0.765464</td>\n",
              "      <td>0.000002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>0.135789</td>\n",
              "      <td>0.871090</td>\n",
              "      <td>0.762040</td>\n",
              "      <td>0.764514</td>\n",
              "      <td>0.763266</td>\n",
              "      <td>0.000002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>0.185312</td>\n",
              "      <td>0.870142</td>\n",
              "      <td>0.775465</td>\n",
              "      <td>0.762221</td>\n",
              "      <td>0.768531</td>\n",
              "      <td>0.000002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "      <td>0.352652</td>\n",
              "      <td>0.868720</td>\n",
              "      <td>0.675506</td>\n",
              "      <td>0.779764</td>\n",
              "      <td>0.708184</td>\n",
              "      <td>0.000020</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   epoch_num  train_loss   val_acc  ...  val_precision    val_f1        lr\n",
              "0          1    0.355991  0.872038  ...       0.783843  0.722881  0.000002\n",
              "3          4    0.152038  0.871090  ...       0.764246  0.765464  0.000002\n",
              "4          5    0.135789  0.871090  ...       0.764514  0.763266  0.000002\n",
              "2          3    0.185312  0.870142  ...       0.762221  0.768531  0.000002\n",
              "5          1    0.352652  0.868720  ...       0.779764  0.708184  0.000020\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OJfsMAdHecX"
      },
      "source": [
        "report_df3 = fine_tuning(config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "id": "0ykqfG5VHKlA",
        "outputId": "c2d3d1f0-2629-4541-fe95-735c80ab9aba"
      },
      "source": [
        "#This report is for the next hyperparameters:\n",
        "\"\"\"\n",
        "epochs = 10\n",
        "lr = 2e-06\n",
        "max_seq_length = 128\n",
        "batch_size = 16\n",
        "\"\"\"\n",
        "report_df3.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>epoch_num</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>val_acc</th>\n",
              "      <th>val_recall</th>\n",
              "      <th>val_precision</th>\n",
              "      <th>val_f1</th>\n",
              "      <th>lr</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>5</td>\n",
              "      <td>0.135143</td>\n",
              "      <td>0.873934</td>\n",
              "      <td>0.752079</td>\n",
              "      <td>0.771473</td>\n",
              "      <td>0.761125</td>\n",
              "      <td>0.000002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>3</td>\n",
              "      <td>0.212011</td>\n",
              "      <td>0.873934</td>\n",
              "      <td>0.742752</td>\n",
              "      <td>0.772983</td>\n",
              "      <td>0.756286</td>\n",
              "      <td>0.000002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>2</td>\n",
              "      <td>0.265641</td>\n",
              "      <td>0.873460</td>\n",
              "      <td>0.727310</td>\n",
              "      <td>0.775144</td>\n",
              "      <td>0.747257</td>\n",
              "      <td>0.000002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0.355991</td>\n",
              "      <td>0.872038</td>\n",
              "      <td>0.691481</td>\n",
              "      <td>0.783843</td>\n",
              "      <td>0.722881</td>\n",
              "      <td>0.000002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>4</td>\n",
              "      <td>0.168793</td>\n",
              "      <td>0.871564</td>\n",
              "      <td>0.768153</td>\n",
              "      <td>0.765073</td>\n",
              "      <td>0.766596</td>\n",
              "      <td>0.000002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>0.152038</td>\n",
              "      <td>0.871090</td>\n",
              "      <td>0.766704</td>\n",
              "      <td>0.764246</td>\n",
              "      <td>0.765464</td>\n",
              "      <td>0.000002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>0.135789</td>\n",
              "      <td>0.871090</td>\n",
              "      <td>0.762040</td>\n",
              "      <td>0.764514</td>\n",
              "      <td>0.763266</td>\n",
              "      <td>0.000002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>6</td>\n",
              "      <td>0.112253</td>\n",
              "      <td>0.871090</td>\n",
              "      <td>0.763206</td>\n",
              "      <td>0.764441</td>\n",
              "      <td>0.763820</td>\n",
              "      <td>0.000002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>0.185312</td>\n",
              "      <td>0.870142</td>\n",
              "      <td>0.775465</td>\n",
              "      <td>0.762221</td>\n",
              "      <td>0.768531</td>\n",
              "      <td>0.000002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "      <td>0.352652</td>\n",
              "      <td>0.868720</td>\n",
              "      <td>0.675506</td>\n",
              "      <td>0.779764</td>\n",
              "      <td>0.708184</td>\n",
              "      <td>0.000020</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    epoch_num  train_loss   val_acc  ...  val_precision    val_f1        lr\n",
              "14          5    0.135143  0.873934  ...       0.771473  0.761125  0.000002\n",
              "12          3    0.212011  0.873934  ...       0.772983  0.756286  0.000002\n",
              "11          2    0.265641  0.873460  ...       0.775144  0.747257  0.000002\n",
              "0           1    0.355991  0.872038  ...       0.783843  0.722881  0.000002\n",
              "13          4    0.168793  0.871564  ...       0.765073  0.766596  0.000002\n",
              "3           4    0.152038  0.871090  ...       0.764246  0.765464  0.000002\n",
              "4           5    0.135789  0.871090  ...       0.764514  0.763266  0.000002\n",
              "15          6    0.112253  0.871090  ...       0.764441  0.763820  0.000002\n",
              "2           3    0.185312  0.870142  ...       0.762221  0.768531  0.000002\n",
              "5           1    0.352652  0.868720  ...       0.779764  0.708184  0.000020\n",
              "\n",
              "[10 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNH9iYGcQzHI"
      },
      "source": [
        "report_df4 = fine_tuning(config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "id": "0kf64Uy2Q9sT",
        "outputId": "9a08ec45-67ee-475a-ce34-4fdd4fd4e5c2"
      },
      "source": [
        "#This report is for the next hyperparameters:\n",
        "\"\"\"\n",
        "epochs = 10\n",
        "lr = 2e-04\n",
        "max_seq_length = 128\n",
        "batch_size = 16\n",
        "\"\"\"\n",
        "report_df4.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>epoch_num</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>val_acc</th>\n",
              "      <th>val_recall</th>\n",
              "      <th>val_precision</th>\n",
              "      <th>val_f1</th>\n",
              "      <th>lr</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>5</td>\n",
              "      <td>0.135143</td>\n",
              "      <td>0.873934</td>\n",
              "      <td>0.752079</td>\n",
              "      <td>0.771473</td>\n",
              "      <td>0.761125</td>\n",
              "      <td>0.000002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>3</td>\n",
              "      <td>0.212011</td>\n",
              "      <td>0.873934</td>\n",
              "      <td>0.742752</td>\n",
              "      <td>0.772983</td>\n",
              "      <td>0.756286</td>\n",
              "      <td>0.000002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>2</td>\n",
              "      <td>0.265641</td>\n",
              "      <td>0.873460</td>\n",
              "      <td>0.727310</td>\n",
              "      <td>0.775144</td>\n",
              "      <td>0.747257</td>\n",
              "      <td>0.000002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0.355991</td>\n",
              "      <td>0.872038</td>\n",
              "      <td>0.691481</td>\n",
              "      <td>0.783843</td>\n",
              "      <td>0.722881</td>\n",
              "      <td>0.000002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>4</td>\n",
              "      <td>0.168793</td>\n",
              "      <td>0.871564</td>\n",
              "      <td>0.768153</td>\n",
              "      <td>0.765073</td>\n",
              "      <td>0.766596</td>\n",
              "      <td>0.000002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>6</td>\n",
              "      <td>0.112253</td>\n",
              "      <td>0.871090</td>\n",
              "      <td>0.763206</td>\n",
              "      <td>0.764441</td>\n",
              "      <td>0.763820</td>\n",
              "      <td>0.000002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>0.135789</td>\n",
              "      <td>0.871090</td>\n",
              "      <td>0.762040</td>\n",
              "      <td>0.764514</td>\n",
              "      <td>0.763266</td>\n",
              "      <td>0.000002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>0.152038</td>\n",
              "      <td>0.871090</td>\n",
              "      <td>0.766704</td>\n",
              "      <td>0.764246</td>\n",
              "      <td>0.765464</td>\n",
              "      <td>0.000002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>0.185312</td>\n",
              "      <td>0.870142</td>\n",
              "      <td>0.775465</td>\n",
              "      <td>0.762221</td>\n",
              "      <td>0.768531</td>\n",
              "      <td>0.000002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>9</td>\n",
              "      <td>0.067870</td>\n",
              "      <td>0.868720</td>\n",
              "      <td>0.745465</td>\n",
              "      <td>0.760946</td>\n",
              "      <td>0.752770</td>\n",
              "      <td>0.000002</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    epoch_num  train_loss   val_acc  ...  val_precision    val_f1        lr\n",
              "14          5    0.135143  0.873934  ...       0.771473  0.761125  0.000002\n",
              "12          3    0.212011  0.873934  ...       0.772983  0.756286  0.000002\n",
              "11          2    0.265641  0.873460  ...       0.775144  0.747257  0.000002\n",
              "0           1    0.355991  0.872038  ...       0.783843  0.722881  0.000002\n",
              "13          4    0.168793  0.871564  ...       0.765073  0.766596  0.000002\n",
              "15          6    0.112253  0.871090  ...       0.764441  0.763820  0.000002\n",
              "4           5    0.135789  0.871090  ...       0.764514  0.763266  0.000002\n",
              "3           4    0.152038  0.871090  ...       0.764246  0.765464  0.000002\n",
              "2           3    0.185312  0.870142  ...       0.762221  0.768531  0.000002\n",
              "18          9    0.067870  0.868720  ...       0.760946  0.752770  0.000002\n",
              "\n",
              "[10 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnZew6ojaKz4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}